
# coding: utf-8

# In[1]:

import numpy as np
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
import csv
import math
from keras.layers import Dense, Flatten, Lambda, Activation, MaxPooling2D, ELU, Dropout
from keras.layers.convolutional import Convolution2D
from keras.models import Sequential
from keras.optimizers import Adam
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import json

num_epochs = 30
batch_size = 64
BRIGHTNESS_RANGE = .25


# In[2]:

def img_change_brightness(img):
    # Convert the image to HSV
    temp = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    # Compute a random brightness value and apply to the image
    brightness = BRIGHTNESS_RANGE + np.random.uniform()
    temp[:, :, 2] = temp[:, :, 2] * brightness

    # Convert back to RGB and return
    return cv2.cvtColor(temp, cv2.COLOR_HSV2RGB)


# In[3]:

def random_gamma(image):
    """
    Random gamma correction is used as an alternative method changing the brightness of
    training images.
    http://www.pyimagesearch.com/2015/10/05/opencv-gamma-correction/
    :param image:
        Source image
    :return:
        New image generated by applying gamma correction to the source image
    """
    gamma = np.random.uniform(0.4, 1.5)
    inv_gamma = 1.0 / gamma
    table = np.array([((i / 255.0) ** inv_gamma) * 255
                      for i in np.arange(0, 256)]).astype("uint8")

    # apply gamma correction using the lookup table
    return cv2.LUT(image, table)


# In[4]:

def random_shear(image, steering_angle, shear_range=200):
    
    randi = np.random.randint(0, 2)
    if randi == 1:
        rows, cols, ch = image.shape
        dx = np.random.randint(-shear_range, shear_range + 1)
        random_point = [cols / 2 + dx, rows / 2]
        pts1 = np.float32([[0, rows], [cols, rows], [cols / 2, rows / 2]])
        pts2 = np.float32([[0, rows], [cols, rows], random_point])
        dsteering = dx / (rows / 2) * 360 / (2 * np.pi * 25.0) / 6.0
        M = cv2.getAffineTransform(pts1, pts2)
        image = cv2.warpAffine(image, M, (cols, rows), borderMode=1)
        steering_angle += dsteering

    return image, steering_angle


# In[5]:

# crop image by percentage
def crop_image(img, top_percent = 0.35, bottom_percent = 0.1):
    
    top = int(np.ceil(img.shape[0] * top_percent))
    bottom = img.shape[0] - int(np.ceil(img.shape[0] * bottom_percent))
    
    return img[top:bottom, :, :]


# In[6]:

# preprocess image: random flip, crop and resize
def porcess_img(img, angle):
    
    img, angle = random_shear(img, angle)
    
    # randomly flip the image
    randi = np.random.randint(0, 2)
    if randi == 0:
        # flip the image
        img   = cv2.flip(img, 1)
        angle = -1.0 * angle
        
    img = random_gamma(img) #img_change_brightness(img)
    
    #crop the image, remove the top
    img = img[55:135, :, :]
    
    # use opencv to resize image to new dimension
    img = cv2.resize(img, (64, 64), interpolation = cv2.INTER_CUBIC)
    
    return img, angle


# In[7]:

# generate training/validation batch
def get_batch(X, batch_size = 64):
    # randomly pickup training data to create a batch
    while(True):
        X_batch = []
        y_batch = []
                
        b_index = np.random.randint(0, len(X), batch_size)
        # randomly selected batch size images and steering angles
        for i in b_index:
            y_angle = float(X[i][3])
            
            # random chosing image from center/left/right camera
            n = np.random.randint(1, 5)
            if n == 1:
                y_angle += 0.25
            elif n == 2:
                y_angle -= 0.25
            else:
                n = 0   # center image has 50% prob to be used
                
            
            img_path = './data/' + X[i][n].strip()
            drv_img  = plt.imread(img_path)
            
            # preprocess image
            drv_img, y_angle = porcess_img(drv_img, y_angle)
            X_batch.append(drv_img)
            y_batch.append(y_angle)
            
        yield np.array(X_batch), np.array(y_batch)


# In[8]:

# create model
def get_model():
    model = Sequential()
    
    # normalization layer
    model.add(Lambda(lambda x: x / 127.5 - 1.0, input_shape=(64, 64, 3)))
    
    # a color map layer the best color map for this hypothesis
    #model.add(Convolution2D(3, 1, 1, border_mode='same'))
              
    # convolution 2D with filter 5x5
    model.add(Convolution2D(24, 5, 5, border_mode='same', subsample=(2, 2)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))
    
    model.add(Convolution2D(36, 5, 5, border_mode='same', subsample=(2, 2)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))
    #model.add(Dropout(0.4))

    model.add(Convolution2D(48, 5, 5, border_mode='same', subsample=(2, 2)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))
    #model.add(Dropout(0.25))

    model.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))
    #model.add(Dropout(0.25))

    model.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))
    model.add(Activation('relu'))
    
    model.add(Flatten())
    
    model.add(Dense(1064))
    model.add(Activation('relu'))
    #model.add(Dropout(0.5))
    
    model.add(Dense(100))
    model.add(Activation('relu'))
    
    model.add(Dense(50))
    model.add(Activation('relu'))
    
    model.add(Dense(10))
    model.add(Activation('relu'))
    
    model.add(Dense(1))
    
    return model


# In[9]:

def get_samples_per_epoch(num_samples, batch_size):
    # return samples per epoch that is multiple of batch_size
    return math.ceil(num_samples/batch_size)*batch_size


# In[10]:

driving_data = []
# create a list of image paths and angles
with open('data/driving_log.csv') as drvfile:
    reader = csv.DictReader(drvfile)
    for row in reader:
        driving_data.append((row['center'], row['left'], row['right'], row['steering']))


# In[11]:

driving_data = shuffle(driving_data)
# split the data, 20% for validation
X_train, X_validation = train_test_split(driving_data, test_size = 0.2, random_state = 7898)


# In[12]:

train_generator = get_batch(driving_data)
val_generator   = get_batch(driving_data)

model = get_model()

model.compile(optimizer = Adam(lr = 0.0001), loss='mse')

print("Start training...")
model.fit_generator(train_generator,
                    samples_per_epoch = 25600, #get_samples_per_epoch(len(X_train), batch_size),
                    nb_epoch = num_epochs,
                    validation_data = val_generator,
                    nb_val_samples = 6400, #get_samples_per_epoch(len(X_validation), batch_size),
                    verbose = 1)

# save model and weights
model_json = model.to_json()
with open("./model.json", "w") as json_file:
    json.dump(model_json, json_file)
model.save_weights("./model.h5")
print("Saved model to disk")


# In[ ]:



